% chktex-file 13

\chapter{Méthodes}
\addChpLof
\label{chp:methodes}
\graphicspath{{resources/méthodes}}

\minitoc%
\clearpage

Maintenant que les objectifs de ce projet ont été précisés, nous détaillons dans ce chapitre l'ensemble des outils nécessaire à leur accomplissement.
Nous décrivons dans les sections qui suivent les données utilisées, puis les méthodes utilisées pour séparer notre zone d'étude en sous"-régions, pour repérer les fronts, et pour extraire nos résultats.

\section{Données}
\label{sec:donnees}

Dans cette section sont décrits les jeux de données utilisés pour ce projet.
Le choix de ces données s'est avéré être une question importante et difficile dès le départ.
Nous savions qu'il allait être nécessaire de combiner plusieurs jeux de données: la SST avec la \as{chl}, puis avec des données de composition de la communauté de phytoplancton.
Cela a donc informé le choix des données, ainsi que la création des outils nécessaires à la gestion de plusieurs sets de données.
Plusieurs options ont été considérées, notamment pour la SST.
Elles sont reportées ci"-dessous, même si elles ne sont pas utilisées pour le reste des résultats présentés.

Au long de ce chapitre (et ailleurs dans ce manuscrit) les articles publiés correspondant aux jeux de données sont bien évidemment cités, mais sont également précisés les accès par lesquels nous avons obtenus les données.
Ces derniers sont cités en note de bas de page et listés dans la section~\reftitle{bib:data} de la bibliographie~(\cpageref{bib:data}).

\subsection{Sea Surface Temperature (SST)}
\label{sec:donnees-sst}

Nous commençons par la température de surface que nous allons notamment utiliser comme proxy de la densité pour repérer les fronts.
De nombreux produits de SST sont disponibles, néanmoins il n'en demeure pas moins difficile de trouver un produit qui satisfasse les exigences inhérentes à notre étude.
Étant donné que nous voulons repérer des structures de fine échelle la résolution est peut être l’exigence qui apparaît comme première.
Cependant, et comme nous allons le voir plus en détail, d'autres paramètres rentrent en compte et il est nécessaire de faire des compromis dans ce domaine.
Brièvement ces autres paramètres sont (entre autres) la disponibilité des autres variables à une résolution similaire, la facilité d'obtention\footnote{%
  Par facilité d'obtention j'entends le téléchargement des données mais aussi et surtout les traitements supplémentaires nécessaires, comme le repérage des pixels nuageux \ab{par-ex}},
ou encore la couverture spatio"-temporelle.

% Éventuellement à déplacer à une mention antérieure des résolutions
\begin{note}[label={note:approx-lat-km}]
  Bien que notre zone d'étude s'étende à de (relativement) hautes latitudes~(\tapprox\latlon{55N}), nous ne considérons pas les différences de distances zonales et méridionales.
  Nous faisons donc une correspondance simple entre fraction de degré et kilomètres, quelque soit la distance considérée, notamment pour la taille des pixels et des fenêtres.
\end{note}

\subsubsection{Modis-L3}
\declareDataset{sst_modis}
\declareDataset{chl_modis}

Afin d'obtenir des données avec une résolution spatiale optimale, une solution est de travailler directement à partir d'un produit \guil*{brut}.
Nous commençons donc par un jeu de données que l'on désignera par \dataname{sst_modis}, s'appuyant sur les données~\abbrv{L2} du capteur \as{modis}, à bord du satellite Aqua (\cite{kilpatrick_2015}).
Ce sont les données utilisées par \textcite{liu_2016} \encadra{et produites par les même outils} dont l'étude consistait également à détecter des fronts de SST et les colocaliser aux données de \as{chl}.

% Éventuellement à déplacer à une mention antérieure des niveaux
\begin{note}[label={note:data-levels}, breakable]
  Tout au long de ce manuscrit nous utilisons les dénominations des niveaux de données tels que typiquement utilisés par les distributeurs, et qui correspondent de manière générale aux définitions suivantes:
  \newcommand*\lvllabel[1]{\hspace\labelsep \normalfont\bfseries\lfstyle #1}%
  \begin{flexlabelled}{lvllabel}{1em}{1.5em}{1em}{2em}{1em}
    \item[L1] Données d'un seul capteur, sans traitement (au delà de ceux nécessaire à la transmission depuis le satellite), et à la résolution de capture \encadra*{\ab{cad} projetées dans le référentiel du capteur (nb.\ de lignes et nb.\ de pixels par ligne)}.
    Elles sont accompagnées des données de positionnement du satellite.

    \item[L2] Variables géophysiques dérivées des données~\abbrv{L1}, à la même résolution et disposés de la même manière que ces dernières.
    Des métadonnées peuvent les accompagner, comme la qualité des pixels par exemple.

    \item[L3] Variables projetées sur une grille régulière en espace et en temps.
    Des traitement supplémentaires peuvent être appliqués.

    \item[L4] Résultats d'analyses plus poussées comme l’agrégation de plusieurs sources de données, ou l'assimilation par un modèle numérique.
  \end{flexlabelled}
\end{note}

Les données sont téléchargées au niveau~\abbrv{L2} depuis le \af{cmr}\footfullcite{sst_modis} en sélectionnant les sections de fauchées\footnotemark{} intersectant notre région d'étude, collectés de jour, et pour les courtes longueurs d'onde infrarouges~(\qty{11}{\um}).
Ces données sont projetées sur une grille kilométrique globale, en reprenant les outils utilisés par \citeauthor{liu_2016}, à savoir les programmes présents dans le paquet de l'\href{https://oceandata.sci.gsfc.nasa.gov/ocssw}{\as{ocssw}}, version~v7.5, qui est maintenu par l'\al{obpg} et notamment distribué à travers l'outil \href{https://seadas.gsfc.nasa.gov/}{SeaDAS}.
Les sections de fauchées, maintenant toutes sur la même grille spatiale, peuvent être moyennées jour par jour, pour obtenir un produit de niveau~\abbrv{L3}.
\footnotetext{%
  \engquote{swath} en anglais, bande de terrain couverte par le satellite le long de son orbite.
  \as{modis} \slashfrac{s}{}\,Aqua produit des fauchées d'environ~\qty{2300}{\km} de large, qui sont distribuées en sections d'environ la même longueur (\ab{cad} dans la direction de la trajectoire).
}

Cela permet donc d'obtenir des données à la limite de la résolution des capteurs, mais présente un certain nombre de désavantages majeurs.
C'est tout d'abord un travail important à réaliser: d'une part de téléchargement, les fichiers~\abbrv{L2} étant particulièrement lourds et nombreux (jusqu'à une dizaine pour une journée); d'autre part de projection, qui à une résolution kilométrique demande une certaine puissance de calcul.
De plus, toutes les étapes décrites jusqu'ici ne concernent qu'un seul capteur.
Des étapes et calculs additionnels seraient nécessaires pour y combiner les données d'autres capteurs, ce qui pose des problèmes supplémentaires et se ferait non sans difficultés.
Même pour un capteur identique (\as{modis} à bord de Terra \ab{par-ex}), il devient nécessaire de considérer des différences de calibrations, ainsi que de possibles artefacts lorsque l'on superpose plusieurs fauchées (que nous avons d'ailleurs largement ignorés pour agréger les fauchées sur une seule journée\dots).

Face à ces difficultés, nous avons donc décidé dans un premier temps d'en rester à un seul capteur, malgré la couverture spatiale réduite.
Il est également à noter que nous n'avons pas réalisé de détection de nuages supplémentaire à celle des données~\abbrv{L2}, qui ne suffit pas à exclure des pixels aux valeurs visiblement erronées (ce qui se manifeste notamment par des valeurs bruitées autour de nuages).
Il est difficile de disqualifier de tels pixels correctement sans un bon algorithme de détection de nuage, ce qui nécessite un travail supplémentaire important.
Nous contournons le problème d'une manière similaire à celle de \citeauthor{liu_2016} en ne travaillant que sur des fenêtres de~\qtyproduct{100 x 100}{\km} avec une faible couverture nuageuse~(\pct{<70}), ce qui élimine une partie de ces cas problématiques.

Malgré le fait d'être limité à deux capteurs (à bords de Aqua et Terra), \as{modis} présente l'avantage de mesurer concomitamment la SST et la couleur de l'océan.
Cela permet d'obtenir les valeurs de \as{chl} aux même pixels, en utilisant le même traitement pour le jeu de données.


Les difficultés évoquées ci-dessus motivent à trouver un produit nécessitant moins de travail (et donc autant d'éventuelles erreurs en moins), ces dernières années ayant vu apparaître des produits globaux distribués à des résolutions élevées.

\subsubsection{MUR}
\declareDataset{sst_mur}

Le produit suivant, désigné ici \af{mur} est développé et distribué par le \as{ghrsst} au JPL \eng{Physical Oceanography} \as{daac}\footfullcite{sst_mur}.
Il présente les avantages d'être distribué à une résolution kilométrique et sans nuages grâce à une méthode d'interpolation par vaguelettes (\cite{chin_2017}).
Il intègre de nombreuses sources provenant de plusieurs capteurs infrarouges et micro"-ondes, ainsi que de mesures in"-situ.
La grande couverture spatiale de ce produit se fait donc par l'utilisation de mesures non"-contraintes par la couverture nuageuse (\ab{cad} micro"-ondes et in"-situ), mais qui présentent une résolution bien plus faible.
Le champ de SST se retrouve donc lissé à la fois par l'interpolation et par l'inclusion de ces mesures.
L'utilisation du produit suivant cherche à palier à ce problème.

\subsubsection{ESA-SST-CCI / \textlf{C3S}}
\declareDataset{sst_esacci}

Ce produit est distribué conjointement par l'\as{esa}"-\abbrv{SST}"-\as{cci} et le \as{c3s}\footnote{%
  l'ESA-CCI pour la période 1981--2016 et le \abbrv{C3S} après 2017}.
Il~agglomère les mesures de tous les capteurs infrarouges disponibles depuis~1981, ce qui comprend 11~\as{avhrr} à bord des satellites \as{metop}["~A] et \as{noaa} (entre les itérations 6 et~19) et trois \as{atsr} à bord de \as{ers} 1~et~2, et Envisat. \review{Ajouter SLSTR.}
Cela donne au moins deux capteurs en fonctionnement simultané, et au moins trois depuis~1992.
Outre la bonne couverture obtenue, l'uniformité des capteurs permet d'obtenir un jeu de données stable dans le temps, pouvant servir à la détection de tendances climatiques~(\cite{merchant_2019}).

Les données sont distribuées à différents niveaux, dont nous allons préciser le passage de l'un à l'autre dans la suite:
\begin{itemize}
  \item \abbrv{L2P} (mono-capteur),
  \item \abbrv{L3U} (mono-capteur sur grille régulière),
  \item \abbrv{L3C} (multi-capteurs regroupés par famille d'instrument),
  \item et \abbrv{L4} (tous les capteurs, combinés par interpolation).
\end{itemize}

Le produit~\abbrv{L2P} est similaire aux données brutes utilisées pour le set \datasect{sst_modis}, mais avec l'avantage de disposer de données standardisées sur un grand nombre de capteurs.
Ces données sont projetées capteur par capteur sur une grille régulière, à une résolution de~\resol{1}{20}, soit environ~\qty{5}{\km}, ce qui donne le produit~\abbrv{L3U}.

\begin{note}
  Nous pourrions réaliser nous même cette projection à une résolution plus élevée, de la même manière que pour le produit \dataname{sst_modis}.
  Nous disposerions alors de données~\abbrv{L3} à plus haute résolution, pour un grand ensemble de capteurs (plus nombreux que les deux disponibles pour \as{modis}).
\end{note}

Les données sont regroupées par famille d'instrument pour le produit~\abbrv{L3C}. Une légère perte de précision est attendue, mais les produits \abbrv{L3U} et~\abbrv{L3C} peuvent néanmoins tous deux être utilisés dans l'étude de structures fines et où la présence de nuages n'est pas rédhibitoire~(\cite{merchant_2019}).

Enfin, le produit~\abbrv{L4} regroupe l'ensemble des capteurs en utilisant le schéma d'intégration variationnelle \abbrv{NEMOVAR} intégré dans le système \as{ostia} (\cite{good_2020}).
Ce produit donne ainsi un champ de SST sans nuages, estimé par une combinaison des observations satellites, et de la prévision d'un modèle numérique d'un jour sur l'autre.
L'absence de couverture nuageuse et la simplicité d'usage se fait au détriment du lissage inévitable des structures les plus fines.

Une estimation quantitative du lissage due à l'interpolation est compliqué, car la paramétrisation de cette dernière dépend de la variabilité du champ de température à~\abbrv{J"~1}.
On peut néanmoins imaginer que le champ de SST produit est plus lissé dans les zones normalement nuageuses, et de plus potentiellement plus éloigné de la réalité (voir \nref{sec:donnees-sst_reanalyses}).
Il est tout de même possible de mitiger ces effets en ne comptabilisant dans nos statistiques seulement les pixels non"-nuageux, ce que nous faisons pour tous les résultats présentés.
Nous utilisons pour cela les données de \as{chl} qui identifient les pixels nuageux.
% À noter qu'il serait également possible d'utiliser les erreurs d'intégration associées à chaque pixel, mais cela n'a pas été exploré.
% -> c'est pas vraiment possible, les erreurs d'inté sont données avec une résolution très faible

\subsubsection{GLORYS}
\declareDataset{sst_reanalyses}

Une solution pour s'affranchir de la couverture nuageuse est de s'appuyer sur des produits de réanalyses.
Tôt dans ce projet, certaines pistes de travail (\ab{par-ex} le suivi temporel des fronts) nous ont poussé à tester l'utilisation de tels produits.
Nous utilisons ici une réanalyse globale (désignée par \dataname{sst_reanalyses}), délivrée par \as{cmems}\footfullcite{sst_reanalyses} et produite par \glshref{mercator}.
Cette réanalyse est faite sur une grille \engquote{eddy"-resolving}, au \resol{1}{12} soit environ~\qty{8}{\km}, et sur 50~niveaux verticaux. Les données sont disponibles en résolution journalière.
Le noyau \abbrv{nemo} est forcé en surface par les réanalyses \abbrv{ERA}-Interim et \abbrv{ERA-5} du centre \as{ecmwf}.
Le modèle fait de l'assimilation de données d'observations: altimétrie, SST \guil{Reynolds}~\resol{1}{4} (seulement \as{avhrr}), concentration en glace de mer, et profiles in"-situ de températures et profils de salinité~\as{cora}.

\subsubsection{Comparaison des produits de SST}

\begin{figure}
  \insertfig{comparaison_sst.pdf}
  \captionT{Comparaison des produits de SST}{%
    Zoom sur une même fenêtre, le \frenchdate{2007}{04}{22}, de la SST pour quatres produits considérés: \dataname{sst_esacci}~(a), \dataname{sst_modis}~(b), \dataname{sst_mur}~(c), et \dataname{sst_reanalyses}~(d).
  }
  \label{fig:comparaison-sst}
\end{figure}

Nous illustrons ici notre comparaison entre les différents produits de SST en nous appuyant sur un instantané (\nref{fig:comparaison-sst}).
Seul le produit \dataname{sst_modis}~(b) n'est pas issu d'une interpolation ou d'une réanalyse, on peut le considérer comme étant le plus proche d'une \engquote{ground truth} (en dehors des pixels nuageux bien sûr).

On notera pour le produit de réanalyses \dataname{sst_reanalyses}~(d), outre sa résolution assez faible, des différences importantes dans les structures présentes sur les autres produits, qui ne sont visiblement pas due à un simple lissage du champ de SST.
Ce produit manque les caractéristiques de certaines structures, mais surtout manque leur position exacte.

Les produits issus d'interpolations de plusieurs capteurs semblent moins souffrir de ce problème.
Les données \dataname{sst_mur}~(c), malgré leur haute résolution présentent des zones très lissées, par exemple dans le secteur~\latlonRange{68}{64W}; \latlonRange{40}{42N}.
On peut émettre l'hypothèse que l'inclusion de données basse-résolution (in"-situ et micro"-ondes) en est (au moins partiellement) la cause.
À l'inverse, les données \dataname{sst_esacci}~(a) semblent capturer avec plus de régularité les structures visibles sur les données plus brutes de \dataname{sst_modis}, excepté les plus fines évidemment.

Le produit \dataname{sst_esacci} semble être le plus approprié parmi ceux présentés.
Il permet d'accéder au champ de SST simplement, ce avec une résolution convenable.
Sauf indication contraire, c'est le produit~\dataname{sst_esacci} que nous utiliserons pour le champ de SST dans toute la suite de cette thèse.

\subsection{Chlorophylle-\textit{a}}
\label{sec:donnees-chl}
\declareDataset{chl_globcolour}

Pour le champ de \al{chl}, seuls deux produits ont été utilisés.
D'une part les données du capteur \as{modis} à bord d'Aqua.
Comme évoqué précédemment ce capteur mesure concomitamment les bandes infrarouges et visibles ce qui permet de bénéficier des valeurs de SST et de \as{chl} aux même points.
Les valeurs de \as{chl} sont calculées à partir des valeurs de réflectance à différentes bandes de longueurs d'ondes, ici avec l'algorithme OCI~(Ocean Colour Index).
Ce dernier combine les algorithmes: CI pour les faibles valeurs de \as{chl}, OCx~(\as{ie} \abbrv{OC3} pour le capteur \as{modis}) pour les fortes valeurs, et un mélange des deux dans une zone de transition~(0.25~\textless \as{chl}~\textless 0.35~\unit{mgm}).
Les données de \as{chl} sont téléchargées séparément, cette fois-ci depuis le \as{cmems}\footfullcite{chl_modis}, mais le même traitement de projection sur grille régulière kilométrique leur est appliquée.

En second lieu, nous utilisons également des données de l'initiative \as{octac} (service du \as{cmems}).
Nous utilisons le produit global, MultiYear\footnote{%
  MultiYear: traitement le plus récent de toutes les années disponibles
}, au niveau~\abbrv{L3}, version dite Copernicus"-GlobColour, développée, validée et distribuée par ACRI-ST France.
Les données sont récupérées sur \as{cmems}\footfullcite{chl_globcolour}.
Elles sont de résolution spatiale~\resol{1}{24} (soit environ~\qty{4}{\km}), journalières, et comportent des nuages.

Ce produit agrège les données optiques de plusieurs capteurs: \as{seawifs}, \as{modis} Aqua et Terra, \as{meris} à bord d'Envisat, \as{viirs} à bord de \as{snpp} et \as{noaa}[\textlf{-20}]\footnote{parfois indiqué par son nom avant lancement: \as{jpss}[\textlf{-1}]}, et enfin \as{olci} à bord de \textlf{Sentinel-3A et -3B}.
Similairement au produit Modis, les valeurs de \as{chl} sont calculées à partir des réflectance en utilisant ici un mélange des algorithmes \abbrv{OC3/4}, CI, et \abbrv{OC5}.

\begin{note}[label={note:merge-chl-or-rrs}]
  Cette aggrégation se fait selon le traitement dit Copernicus"-GlobColour: les données de reflectance de chaque capteur sont transformées en concentration de \as{chl} avant d'être fusionnées en seul produit~(\cite{garnesson_2019}).
  À l'inverse, le produit (\as{esa})~OC-\as{cci}/\as{c3s} du même projet \as{octac}, développé par le Plymouth Marine Laboratory, fusionne les reflectances des différents capteurs avant la conversion en \as{chl}.
  Aggréger les valeurs de Chlorophylle plutôt que de réflectance permet \encadra{entre autres} un ajout de nouveaux capteurs facilité (à ce jour le produit OC-\as{cci} n'intègre pas le capteur de \as{noaa}[-\textlf{20}] par ex.), ainsi qu'une meilleur couverture spatiale~(\cite{garnesson_2019}).
\end{note}

Produits vont par pairs.
Autant garder ceux à 4km.
Ce produit présente essentiellement les même avantages et inconvénients que son homologue pour SST.

\begin{figure}
  \centering
  \insertfig{comparaison_chl.pdf}
  \captionT{Comparaison des produits de Chl-\textit{a}}{%
    Zoom sur une même fenêtre, le \frenchdate{2007}{04}{22}, de la \as{chl} pour les deux produits considérés: Copernicus"-GlobColour~(a), et \abbrv{Modis-L3}~{b}.
  }
  \label{fig:comparaison-chl}
\end{figure}

\subsection{Accord entre les produits de SST et Chl-\textit{a}}

Il est à noter qu'il est difficile de trouver des produits de \al{chl} à des résolutions supérieures à~\qty{4}{\km}, en tout cas au niveau global.
Cela motive à utiliser un produit SST de résolution légèrement moindre afin d'éviter une étape supplémentaire de \engquote{downsampling} une fois les fronts repérés sur le champ de SST.
Cela renforce notre choix pour les données \dataname{sst_esacci}, en défaveur du set \dataname{sst_mur}.

Les deux jeux choisis pour la SST et \as{chl} ont des résolutions spatiales similaires mais néanmoins légèrement différentes.
Les deux grilles sont plate"-carrées (régulières en latitude et longitude) mais le champ de \as{chl} est défini sur une grille \glshref{epsg-chl} de résolution~\resol{1}{24}~(\tapprox\qty{4.6}{\km}); et le champ de SST sur une grille \glshref{epsg-sst} de résolution~\resol{1}{20}~(\tapprox\qty{5.6}{\km}).

Sachant que les produits de composition du phytoplancton dont nous disposions (non décrits ici) sont définis sur la même grille que la Chlorophylle, nous adaptons la SST sur la grille de la \as{chl} par une simple interpolation bi"-linéaire.

\subsection{Bathymétrie}
\label{sec:donnees-bathymetrie}
\declareDataset{etopo1}

Nous utilisons les données de bathymétrie \abbrv{ETOPO1}\footfullcite{etopo1}, fournies par \as{noaa}.
Les données sont distribuées sur une grille plate"-carrée à une résolution d'une minute d'arc~(\resol{1}{60}), soit trois fois trop pour nous.
Nous sous"-échantillonnons les données en appliquant une moyenne à chaque groupe de \qtyproduct{3x3}{\pixels}, puis en interpolant le résultat sur la même grille que le reste des données de la même manière que pour la SST.
La bathymétrie nous permet d'exclure de nos résultats le plateau continental (\nref{fig:bathymetrie}), pour des raisons discutées ci"-dessous.

\begin{figure}
  \centering
  \insertfig[0.6]{bathymétrie.pdf}
  \captionT{Bathymétrie de la zone d'étude}{%
    L'isobath~\qty{1500}{\m}, indiqué en rouge (les Bahamas sont exclues par visibilité), permet de repérer le talus continental et d'ignorer le plateau dans nos résultats.
  }
  \label{fig:bathymetrie}
\end{figure}

\section{Délimitations des biomes, ou sous-régions d'étude}
\label{sec:delimitations-regions}

Comme détaillé plus en profondeur en introduction (\nref{sec:region-detude}, \cref*{chp:introduction}) notre région d'étude est fortement hétérogène, aussi bien concernant les propriétés physiques que biologiques.
Il est ainsi nécessaire de séparer notre zone d'étude en (sous-)régions afin d'extraire des résultats sur des zones homogènes.
Nous définissons donc trois biomes.
Le biome subtropical permanent, le plus au sud, correspond à un régime largement oligotrophe.
Le biome subpolaire, le plus au nord, comprend les eaux froides et productives au nord du Gulf Stream.
Enfin le biome subtropical saisonnier, entre les deux précédents, présente un régime intermédiaire: oligotrophe mais avec une production plus élevée permise par une couche de mélange profonde en hiver.
Dans cette section, nous décrivons plus précisément la méthode utilisée pour définir ces biomes spatialement.

\subsection{Séparation des biomes}

La séparation entre les deux biomes subtropicaux est faite par une limite zonale fixée à~\latlon{32N} (trait noir pointillé \nref{fig:regions}).
Cette limite correspond approximativement à un saut visible des valeurs de \as{chl} à cette latitude (isocontour~\qty{0.1}{\mgm} de la moyenne annuelle de \as{chl}) qui ne varie que peu au cours de l'année.
Cette séparation est en accord avec la limite entre les biomes tels que délimités par \textcite{sarmiento_2004}~(\nref{fig:sarmiento}).

\begin{figure}
  \centering
  \insertfig{régions.pdf}
  \captionT{Résultat de la séparation de la zone d'étude en biomes}{%
    La zone est découpée en trois biomes: le biome subtropical permanent~(PSB) au sud de du trait pointillé à~\latlon{32N}; le biome subtropical saisonnier~(SSB) entre~\latlon{32N} et la limite (sinueuse) nord du Gulf Stream dénotée par le contour noir; et le biome subpolaire~(SP) au nord du Gulf Stream.

    Clichés de SST~(a) et \as{chl}~(b) au \frenchdate{2007}{04}{22}, et les distributions de SST~(c) et (d)~\as{chl} pour chacuns des biomes, le même jour (subpolaire:~bleu, subtropical saisonnier:~jaune, subtropical permenant:~rouge).
    La ligne noire verticale sur~(c) marque la température détectée comme seuil de la limite nord du Gulf Stream.
    Les axes des abscisses des distributions correspondent aux barres de couleurs.
    La ligne rouge suit l'isobath~\qty{1500}{\m}. Le plateau continental~(\qty{<1500}{\m}) n'est pas considéré et est masqué.
  }
  \label{fig:regions}
\end{figure}

\begin{figure}
  \centering
  \insertfig[0.7]{sarmiento_2004_fig2b.png}
  \captionT{Délimitations des biomes par des processus physiques}{%
    Figure tirée de \textcite[figure 2b]{sarmiento_2004}.
    Les biomes de notre zone d'étude correspondent dans la nomenclature de cet article à: subpolaire~(``SP'', jaune); subtropical saisonnier~(``ST-SS'', bleu); et subtropical permanent~(``ST-PS'', rose).

    \foreignblockquote{english}{Biome classification scheme calculated using mixed layer depths obtained from observed density and from upwelling calculated from the wind stress divergence using observed winds.
    The equatorially influenced biome covers the area between \latlon{5S}~and~\latlon{5N}, and is colored a dirty light blue in areas where upwelling occurs (labeled~``Eq-U'' on the color bar) and dark pink in areas where downwelling occurs (labeled~``Eq-D'').
    Outside of this band, the region labeled ``Ice''~(red) is the marginal sea ice biome, the region labeled ``SP''~(yellow) is the subpolar biome, the region labeled ``LL-U''~(light blue) is the low-latitude upwelling biome , the region labeled ``ST-SS''~(dark blue) is the seasonally mixed subtropical gyre biome, and the region labeled ``ST-PS''~(pink) is the permanently stratified subtropical gyre biome.}
  }
\label{fig:sarmiento}
\end{figure}

On sépare ensuite le reste de la région au nord de~\latlon{32N} en prenant comme limite le front nord du jet du Gulf Stream (le \engquote{North wall}).
Cette délimitation est donc dynamique et déterminée chaque jour à partir de l'image de température.
En effet, il apparaît que la distribution de la SST (au nord de~\latlon{32N}) suffit à repérer de manière fiable et robuste une température seuil permettant de séparer les deux biomes (contour sinueux noir plein \nref[figure]{fig:regions}).
Sur cette distribution apparaît clairement un pic dans des valeurs élevées correspondant au eaux du jet.
Il est aisé de repérer le pic et l'ajuster par une gaussienne.
À~partir de là, la température seuil entre les biomes est prise comme la base froide de ce pic, \ab{cad} plus précisemment en soustrayant à la température moyenne du pic deux fois son écart"-type (\nref{fig:temp-seuil-distrib}).
La valeur journalière de ce seuil est filtrée temporellement par un filtre médian glissant (avec une fenêtre de largeur 8~jours) afin d'éviter d'éventuelles anomalies de détection.

\begin{figure}
  \centering
  \insertfig[0.7]{zone_separation.pdf}
  \captionT{Délimitation des biomes subtropical permanent et subpolaire par température seuil}{%
    Sur la distribution des valeurs en température au nord de~\latlon{32N}~(en noir) pour le \frenchdate{2007}{04}{22}, le pic \encadra{ici autour de~\tC{18}} correspond aux eaux du jet. Il est ajusté par un fit gaussien (en rouge) dans un intervalle de~\tC{5} de large.
    La température de seuil~(en bleu) entre les deux biomes est définie comme la température moyenne du pic~(trait fin pointillé) moins deux fois son écart"-type.
    C'est ainsi que nous définissons la limite nord du Gulf Stream.
  }
  \label{fig:temp-seuil-distrib}
\end{figure}

\begin{figure}
  \insertfig{separation_evol_month.pdf}
  \captionT{Variation temporelle de la délimitation entre biomes}{%
    La position limite entre les biomes subtropical saisonnier et subpolaire n'évolue que peu au cours de l'année vis à vis des larges méandres du Gulf Stream.
    Néanmoins en été, la limite est moins marquée après le détachement du jet~(\latlonRange{75}{70W}).

    L'isotherme séparant les deux biomes est tracé le 15~de chaque mois pour l'année~2007.
    La couleur de chaque contour correspond au jour de l'année comme dénoté par la barre de couleur dans l'inset.
    Dans l'inset est tracé la température de seuil au long de l'année~2007. Les cercles marquent les jours et températures utilisées pour chaque contour.
  }
  \label{fig:var-delim}
\end{figure}

\subsection{Exclusion du plateau continental}

Par ailleurs, il est également nécessaire d'éviter de considérer les pixels trop près des côtes dans notre étude.
D'une part la \as{chl} atteint dans les kilomètres les plus proches de côtes des valeurs très élevées (jusqu'à~\qty{10}{\mgm}) pour des raisons qui ne concernent pas notre étude et qui pourraient fausser les distributions mesurées.
D'autre part, nous cherchons à éviter deux zones qui ne correspondent pas aux biomes définis plus haut.

Premièrement, le jet du Gulf Stream prend forme au sud de notre zone, le long de la côte de Floride. On trouve donc sur le plateau continental à ces latitudes~(\tapprox\latlon{28N}) de forts courants qu'on ne retrouve pas dans le reste du biome subtropical permanent.
Deuxièmement, plus au nord dans l'anse Nord-Est Américaine de l'Atlantique (\engquote{Mid"-Atlantic Bight}), on trouve une séparation nette entre les eaux au nord du Gulf Stream (la \engquote{slope sea}), et les eaux sur le plateau continental.
Un jet marque cette séparation le long du talus continental~(\cite{flagg_2006}).

Dans les deux cas, imposer une limite haute à la bathymétrie sur notre région d'étude permet de supprimer ces zones problématiques.
Ainsi, pour calculer nos résultats, nous ne considérons que les pixels où la profondeur n'excède pas~\qty{1500}{\m}.
Nous utilisons pour cela les données de bathymétrie \dataname{etopo1}~(voir \datasect{etopo1}).

\section{Heterogeneity Index (HI)}
\label{sec:HI}

Comme précisé en introduction, pour quantifier l'effet des fronts sur le phytoplancton il est nécessaire de détecter les fronts, \al{cad} de classer chaque pixel comme appartenant à un front ou non (\ab{cad} à l'arrière"-plan ou au \engquote{background}).
La méthode retenue ici suit celle présentée par \textcite{liu_2016} \encadra*{qui par son utilisation d'une fenêtre glissante s'apparente elle même à celle de \textcite{cayula_1992}}.
Cette section définit cette méthode et notre implémentation, tout en indiquant les modifications que nous y avons apportées.

La méthode de \textcite{liu_2016} consiste à quantifier plusieurs quantités statistiques du champ de SST dont les fortes valeurs sont associées à la présence de fronts et autres structures de fine échelle.
Ces quantités sont la bimodalité, l'écart"-type (qui reflète le gradient du champ), et le coefficient d'asymétrie~(\engquote{skewness}).
Ces composantes sont ensuite réunies dans un seul index qui ainsi reflète l'hétérogénéité du champ de SST, et donc baptisé \af{hi}.

Pour limiter la taille des structures détectées, on limite le calcul de ces composantes sur une fenêtre de taille appropriée, \al{cad} pour nous de l'ordre de grandeur d'une dizaine de kilomètres.
Ainsi, pour chaque pixel, la valeur de chaque composante est calculée sur la distribution de SST à l'intérieur d'une fenêtre glissante centrée sur ce pixel et dont les tailles possibles sont:
\begin{itemize}
  \item \(3 \times 3 =\qty{9}{\pixels}\) soit~\tapprox\qty{20}{\km} de côté,
  \item \(5 \times 5 =\qty{25}{\pixels}\) soit~\tapprox\qty{30}{\km} de côté,
  \item \(7 \times 7 =\qty{49}{\pixels}\) soit~\tapprox\qty{40}{\km} de côté.
\end{itemize}
Parce qu'une fenêtre plus large entraînerait la détection de structures trop grandes pour notre étude, nous nous limiterons aux tailles présentées ci"-dessus.
Néanmoins une fenêtre trop petite limite le nombre de pixels disponibles pour calculer les valeurs statistique dont nous avons besoin convenablement.
Un compromis est à définir.

\subsection{Implémentation: Calcul des composantes}
\label{sec:calcul-composantes}

Nous définissons maintenant plus en détails le calcul des composantes du HI.
Pour chaque position de la fenêtre, on s'intéresse aux~\(N\) valeurs de SST~\(s_{i}\) valides (\ab{cad} des provenant des pixels sans nuages).

On commence par l'écart"-type~\as{std}, calculé simplement par:
\begin{equation}
  \am{std} = \sqrt{\frac{1}{N-1} \sum_i \paren{s_i - \moy{s}}^2},
\end{equation}
avec~\(\moy{s}\) la moyenne des valeurs de température.

Ensuite, le coefficient d'asymétrie~\as{skew}, est défini comme la valeur absolue du moment d'ordre trois d'une variable centrée réduite, et qui se calcule donc par:
\begin{equation}
  \am{skew} = \abs{\frac{\sum_i \paren{s_i - \moy{s}}^3} {N \sigma^3}}.
\end{equation}

Enfin, on cherche à quantifier la bimodalité~\as{bimod} de la distribution des valeurs de SST.
Pour ce faire on compare l'écart entre ladite distribution et une distribution gaussienne dont la moyenne et l'écart"-type sont pris identiques à ceux de la SST (\nref{fig:bimodality}).
Cela présuppose que lorsque les températures sont uni"-modales (\ab{cad} quand il n'y pas de fronts dans la fenêtre) leur distribution tend vers une gaussienne, ce qui ne paraît pas déraisonnable.

\begin{figure}
  \centering
  \insertfig[0.6]{bimodality.pdf}
  \captionT{Illustration du calcul de la bimodalité}{
    On calcule la bimodalité comme la norme de la différence entre l'histogramme des températures dans la fenêtre~(trait noir) et une distribution gaussienne de même moyenne et de même écart"-type~(trait rouge).
  }
  \label{fig:bimodality}
\end{figure}

De manière plus précise on commence par calculer l'histogramme~\(h_i\) des valeurs de SST dans la fenêtre, en utilisant des intervalles de largeur fixe de~\tC{0.1} et compris entre les valeurs minimales et maximales dans la fênetre.
Les données de SST étant distribuées compressées par \engquote{linear packing}\footnotemark\ avec un facteur d'échelle de~\tC{0.01}, nos intervalles ont une largeur précisement égale à dix fois l'intervalle de discrétisation des valeurs de température.
Pour éviter que trop de valeurs tombent sur les bords des intervalles et que l'histogramme soit pollué par des erreurs numériques, on décale les intervalles de~\tC[parse-numbers=false]{0.01/2}.
Par ailleurs, le nombre d'intervalles étant dépendant de la largeur de la distribution de SST, dans les cas où il est inférieur ou égal à quatre, la bimodalité est automatiquement assignée nulle~(\(B=0\)).
\footnotetext{%
  Cette technique de compression avec pertes \encadra{utilisée notamment par l'outil \citesoft{nco}\footnotemark{}} consiste à discrétiser des valeurs flottantes sur des entiers après une transformation linéaire.
  Par exemple, en prenant pour stockage des entiers non"-signés sur 16~bits~(\textsf{NC\_USHORT}) on peut évidement représenter des valeurs entières entre \num{0} et~\(2^{16}-1 = \num{65535}\); mais en multipliant ces valeurs entières par un facteur de, disons,~\num{0.005} on peut représenter des valeurs entre \num{0} et~\num{327.675}.
  On a gagné en volume par rapport à un stockage typique de 32 ou~64~bits, mais en perdant évidemment en précision puisque nos valeurs sont maintenant discrétisées avec un intervalle de~\num{0.005}.}
\footnotetext{voir le guide utilisateur:\\\glsurl{nco-packed}}

Par ailleurs on définit une distribution gaussienne~\(g_i\) sur les même intervalles en utilisant les statistiques calculées précédemment:
\begin{equation}
  g_i = \frac{1}{\sqrt{2\pi\am{std}}} \exp\paren{-\frac{\paren{x_i-\moy{s}}^2}{2\am{std}^2}},
\end{equation}
pour ensuite calculer la norme~\(\mathbb{L}^2\) entre les deux distributions:
\begin{equation}
  \begin{split}
  \am{bimod} & = \norme[2]{h - g}\\
             & = \sum_i \paren{h_i - g_i}^2 .
  \end{split}
\end{equation}

Ce principe de calcul de bimodalité s'apparente à celle de \textcite{cayula_1992}, qui consiste à trouver une valeur seuil séparant l'histogramme des valeurs en deux classes et pour laquelle la variance intra"-classe est minimale.
Cette méthode est d'ailleurs utilisée en analyse d'image, non pas sur une fenêtre glissante mais pour toute l'image, et connue sous le nom de méthode d'\textcite{otsu_1979}.
Bien qu'elle soit appliquée avec succès pour la détection de front, il apparaît que cette méthode nécessite de construire un histogramme d'une résolution suffisante, ce qui s'avère difficile aux échelles où nous travaillons.

Notre méthode de calcul de la bimodalité est la première grande modification apportée à celle de \textcite{liu_2016}.
Ces dernier·ère·s calculent également l'histogramme de la température, mais au lieu de directement le comparer avec une distribution gaussienne, iels ajustent l'histogramme par un polynôme de degré~5 avant de comparer ce dernier avec la gaussienne.
Cet ajustement, par ailleurs difficile à mettre en place, est facilement mal conditionné et rien ne garantit sa convergence.
Nous proposons donc une méthode plus facile à implémenter, plus robuste, et au coût de calcul réduit.

\subsection{Implémentation: Coefficients de normalisation}
\label{sec:coef-normalisation}

Avant de pouvoir réunir les trois composantes il est nécessaire de leur appliquer un poids statistique équivalent à chacune. Cela est accompli par calcule de coefficients constants de normalisation.
Alors que \citeauthor{liu_2016} proposent de normaliser chaque composante~\(C^n\) par ses valeurs minimales et maximales (prises sur toutes les valeurs disponibles) selon:
\begin{equation}
  \norm{C_i^n} = \frac {C_i^n - \min(C^n)} {\max(C^n) - \min(C^n)} ;
\end{equation}
nous préférons normaliser par l'écart"-type:
\begin{equation}
  \norm{C_i^n} = \am{coef}^n C_i^n
  \text{ avec } \am{coef}^n = \frac {1} {\std(C^n)} .
\end{equation}
En effet un rapide coup d’œil aux distributions des composantes (\nref{fig:distrib-composantes}) permet de constater qu'aucune des composante ne semble avoir des valeurs bornées.
Normaliser par les valeurs maximales donne d'une part une normalisation arbitraire, très sensible aux valeurs extrêmes, et d'autre part donne un poids disproportionné aux valeurs élevées dans la normalisation.
En revanche, la normalisation par l'écart"-type permet d'attribuer une part équivalente de la variance du HI à chaque composante.

\begin{figure}
  \centering
  \insertfig[0.7]{distrib_composantes.pdf}
  \captionT{Distribution des valeurs de HI et de ses composantes}{%
    Densités de probabilité des valeurs de HI~(a) et de ses composantes: l'écart"-type~(b), le coefficient d'asymétrie~(c), et la bimodalité~(d), calculées sur l'année 2007.
    Chacune des composante s'est vue appliquée son coefficient de normalisation, mais pas le HI, qui est donc ici la simple somme de ces composantes.
  }
  \label{fig:distrib-composantes}
\end{figure}

Jusqu'ici nous n'imposons aucune contrainte sur l'amplitude des valeurs du HI.
Afin de \guil*{standardiser} quelque peu ses valeurs finales, nous définissons un quatrième coefficient de normalisation défini de sorte que~\pct{95} des valeurs du HI soient inférieures à~9.5.

Les coefficients de normalisation~\(\am{coef}^n\) sont obtenus par analyse des distributions des composantes pour l'année~2007. Ils sont ensuite appliqués de manière uniforme au reste des données.

On peut donc calculer le HI avec:
\begin{equation}
  \am{hi} = \am{coef}^4 \paren{
    \am{coef}^1\am{std}
    + \am{coef}^2\am{skew}
    + \am{coef}^3\am{bimod}}.
\end{equation}

On obtient un indice capable de quantifier l'hétérogénéité du champ de température à une échelle donnée, ce qui nous permet d'identifier des structures fines dans ce même champ (\cref{fig:exemple-composantes,fig:exemples-fronts}).
Il reste toutefois à définir une méthode pour classifier chaque pixel comme appartenant, ou non, à un front.
Nous allons même plus loin et séparons les pixels en trois catégories: les pixels appartenant à un front fort~(\(\am{hi} > 5\)), à un front faible~(\(5 < \am{hi} < 10\)), ou à aucun front~(\(\am{hi} < 5\), aussi dénotés \engquote{background}).
La pertinence des valeurs de seuils entre les catégories est discutée dans la suite du manuscrit.

Nous avons réalisé des test de sensibilité à certains paramètres de cette méthode \encadra{à savoir la taille de la fenêtre glissante et le choix des coefficients de normalisation} qui sont discutés plus en détail \nref[section]{sec:sensibilite-parametres}.

\begin{figure}
  \centering
  \insertfig{exemple_composantes.pdf}
  \captionT{Exemples de construction du HI à partir de ses composantes}{%
    Champ de SST~(a) pour le \frenchdate{2007}{04}{07}, à partir de laquelle on calcule les composantes du HI: l'écart"-type~(c), l'asymétrie~(e), et la bimodalité~(f) (ici toutes représentées normalisées par leur variance).
    Le HI~(b) est ensuite obtenu par la somme de ces composantes normalisées, avant d'être normalisé lui aussi afin que~\pct{95} de ses valeurs soient inférieures à~9.5.
    Le HI permet de détecter les fronts de SST, ici deux valeurs du HI normalisé sont contourées, à 5~(trait plein) et 10~(trait pointillé).
  }
  \label{fig:exemple-composantes}
\end{figure}

\begin{figure}
  \centering
  \insertfig{exemples_fronts.pdf}
  \captionT{Exemples de structures fines}{%
    Champs de SST~(colonne gauche), \as{chl}~(colonne centre), et HI~(colonne droite) pour trois exemples de structures: \textlf{1}\ier~exemple~(1a--c) le \frenchdate{2007}{04}{07}, \textlf{2}\ieme~exemple~(2a--c) le \frenchdate{2007}{02}{23}, et \textlf{3}\ieme~exemple~(3a--c) le \frenchdate{2007}{02}{28}.
    Chaque fenêtre représente une surface d'environ~\qtyproduct{200x200}{\km}.
    Deux valeurs seuil de HI sont contourées, à 5~(trait plein) et 10~(trait pointillé).
  }
  \label{fig:exemples-fronts}
\end{figure}


\section{Compression des calculs par utilisation d'histogrammes}
\label{sec:extraction-hist}

Une fois le HI calculé, il devient possible de catégoriser chaque pixel par la biome auquel il appartient (subtropical permanent, subtropical saisonnier, subpolaire), ainsi que par sa valeur de HI.
On cherche ensuite à extraire des informations de ces ensembles de pixels ainsi constitués.
Le nombre total de pixels étant conséquent, et parce que l'établissement des ensembles est compliqué et coûteux, les ensembles de pixels sont chacun réduits à des histogrammes de variables d'intérêt (SST, \as{chl}, ou les \asp{pft}).
Cela diminue les étapes de calcul ainsi que la quantité de données à traiter pour obtenir un diagnostic.

Prenons l'exemple d'une seule image. Pour notre région d'étude, cela représente \qtyproduct{1000 x 1000}{\pixels}.
On cherche à extraire un simple diagnostic, par exemple la moyenne de la \as{chl} dans, et hors des fronts pour chacun des biomes.
Il nous faut donc séparer la région en biomes, ce qui nécessite rappelons-le de discriminer les pixels par leur température (\nref{sec:delimitations-regions}).
Il faut également discriminer les pixels par leur valeur de HI pour séparer fronts et \eng{background}.
On peut maintenant calculer nos statistiques sur chacun des ensembles de pixels constitués, ce qui représente ici de faire 6~calculs (\textlf{3}~biomes~\texttimes\ fronts/\eng{backgound}) sur environ~\qty{e6}{\pixels}.
Mais une fois nos histogrammes calculés, un diagnostic ne requiert que de regarder le ou les histogrammes appropriés, et ce qui représente beaucoup moins d'efforts \encadra*{et de données comme nous allons le voir}.

Les histogrammes peuvent être rendus représentatifs sans pour autant utiliser un nombre prohibitif d'intervalles, d'autant plus que les données (\as{chl} et SST) sont déjà stockées compressées avec pertes. Pour la température par exemple, 450~intervalles suffisent à couvrir toutes les valeurs (de \tC{-5} à~\tC{40}), avec une largeur d'intervalle de~\tC{0.1} équivalente à l'incertitude sur la mesure.
Les intervalles pour la \as{chl} et les autres variables biologiques sont pris de largeur logarithmique afin de couvrir les plusieurs ordres de grandeur que peuvent prendre leurs valeurs.

Les histogrammes présentent également l'avantage de pouvoir facilement être combinés entre eux.
En effet, tous les histogrammes calculés sont stockés non"-normalisés, c'est-à-dire en nombre de pixel par intervalle. Ainsi plusieurs histogrammes peuvent être sommés entre eux avant d'être normalisés pour en extraire une valeur, comme la valeur médiane de la distribution résultante par exemple.
Ce procédé est notamment utilisé pour calculer des diagnostics sur des périodes de temps autres que journalières, sans avoir besoin de refaire un calcul coûteux impliquant les pixels.

Dans la suite on détaille le processus de normalisation des histogrammes.
On considère un histogramme qui compte~\(h_i\) valeurs d'une variable quelconque~\(x\), pour le i\ieme{}~intervalle~\(\left[x_i; x_{i+1} \right]\).

Pour un histogramme donné (\ab{par-ex} pour une région à une date et un type de front donné), on extrait facilement le nombre de pixels total:
\begin{equation}
  N = \sum_i h_i,
\end{equation}
ou une approximation de la valeur moyenne:
\begin{equation}
  \moy{x} = \frac{\sum_i h_i x_i} {N}.
\end{equation}

Pour d'autres valeurs à extraire il est nécessaire de normaliser les histogrammes afin d'obtenir une densité de probabilité. On prendra soin de considérer les tailles des intervalles dans les calculs.
La largeur des intervalles est~\(w_i = x_{i+1}-x_i\). On transforme le nombre de points en probabilité par unité de valeur~\(p_i = h_i / w_i \), avant de le normaliser de sorte à obtenir une intégrale égale à 1:
\begin{equation}
  f_i = \frac{p_i} {\sum_j p_j w_j},
\end{equation}
pour ainsi obtenir une approximation de la densité de probabilité~\(f\).

On peut extraire de cette distribution notamment la médiane, ainsi que des percentiles divers en trouvant la valeur de~\(x\) pour laquelle la somme cumulée de la densité de probabilité est égale au percentile recherché (0.5 dans le cas de la médiane).
On délègue ce travail au paquet SciPy dont l'objet~\glshref{rv_histogram} permet d'effectuer ces calculs de manière triviale.

Il est à noter que l'interprétation de certaines métriques n'a de sens que si la distribution de la variable concernée est convenable (uni"-modale \ab{par-ex}).
Cette vérification est reportée dans la \nref[section]{sec:complements-chl}, \cref*{chp:res-chl}.

\section{Quantification de l'effet des fronts: différences de valeurs}
\label{sec:extraction-surplus-lag}

Afin de comparer les valeurs de diverse variables à l'intérieur et à l'extérieur des fronts, nous définissons deux métriques.
La première est l'excès~\as{exces} de \as{chl} (\engquote{excess} dans l'article \nref[section]{sec:article-bg}, \cref*{chp:res-chl}), qui compare localement les valeurs dans les fronts et le background. On le définit simplement comme la différence relative entre la médiane de \as{chl} dans et hors des fronts:
\begin{equation}
  \am{exces} = \frac{\med{\am{chl}}_\frt - \med{\am{chl}}_\bkg}
  {\med{\am{chl}}_\bkg} .
\end{equation}
Le calcul est fait pour les fronts faibles et forts de la même manière, le background désignant toujours les pixels de HI faible~(\(\am{hi} < 5\)).
L'excès est calculé dans des bandes de latitudes larges de~\ang[mode=math]{5}, afin de minimiser l'influence des gradients de grande échelle.

Cette métrique ne tient compte que de la distribution des valeurs de \as{chl}. Elle ignore la proportion de fronts dans une zone donnée, et ne représente pas la quantité totale de \as{chl} présente dans un biome.
On définit donc une deuxième métrique, le surplus~\as{surplus} de \as{chl} dans tout un biome (\enquote{biome surplus} dans l'article \nref[section]{sec:article-bg}, \cref*{chp:res-chl}) comme la différence relative entre la moyenne de \as{chl} dans tout le biome et la moyenne dans le background:
\begin{equation}
  \am{surplus} = \frac{\moy{\am{chl}}_\tot - \moy{\am{chl}}_\bkg}
  {\moy{\am{chl}}_\bkg} .
\end{equation}

\section{Décalage du bloom}
\label{sec:decalage-bloom}

L'évolution temporelle de la médiane de \as{chl} dans le biome subpolaire présente un bloom printanier dont nous mesurons la phénologie \encadra{plus précisément la date de démarrage et la durée du bloom} à la fois dans le \eng{background} et les fronts.
Étant donné que le bloom se propage vers le nord, ces mesures sont faites sur des bandes de latitudes de largeur~\ang[mode=math]{5}.

Pour extraire ces mesures, nous prenons la série temporelle de la médiane de \as{chl}, à laquelle nous appliquons un filtre Butterworth d'ordre~2 et de fréquence de coupure~\qty[parse-numbers=false]{1/20}{\jours^{-1}}.
La série filtré montre de fortes variations dans leur phénologie d'une année sur l'autre, mais un bloom est toujours discernable.
Seuls les données de février à juillet sont considérées ce qui permet d'isoler le bloom printanier et d'exclure le bloom automnal.
Nous utilisons également la dérivée de \as{chl} que nous calculons à partir de la série filtrée\footnote{%
  La dérivée est également ré-échantillonnée par interpolation cubique à une résolution temporelle de 3~heures, ce qui permet de repérer plus précisement le maximum de dérivée sans avoir à passer par une régression.
}.

Premièrement, nous détectons la valeur maximale de \as{chl} (étape\,\textcircled{1}, en orange, \nref{fig:methode-timing-bloom}a).
Deuxièmement, nous prenons pour démarrage du bloom l'instant~\as{onset}, antérieur à l'instant de \as{chl} maximale, pour lequel la dérivée de \as{chl} est maximale (étape\,\textcircled{2}, en rouge, \nref{fig:methode-timing-bloom}b).
Afin d'estimer l'incertitude de cette mesure, nous prenons l'écart"-type de l'ensemble des dates pour lesquels la dérivée de \as{chl} est supérieur à~\pct{90} de sa valeur maximale.

\begin{figure}
  \centering
  \insertfig{phenologie_méthode_2015_N:45-50.pdf}
  \captionT{Mesure du timing du bloom entre fronts et background}{%
    Les dates de démarrage du bloom et sa durée sont obtenues à partir des séries temporelles de la médiane de \as{chl}~(a) et de sa dérivé~(b). Les données proviennent de l'année~2015 et la bande de latitude~\latlonRange{40}{45N}.

    Le processus est illustré sur la série correspondant aux fronts faibles:
    \textcircled{1}\,Nous déterminons l'instant pour lequel la \as{chl} est maximale, puis \textcircled{2}\,prenons pour démarrage du bloom l'instant antérieur à ce dernier et pour lequel la dérivée de \as{chl} est maximale.
    Enfin\,\textcircled{3} la date de fin de bloom est prise comme la date la plus tardive pour laquelle la \as{chl} est supérieure à~\pct{75} de sa valeur maximale.
  }
  \label{fig:methode-timing-bloom}
\end{figure}

\begin{figure}
  \centering
  \insertfig{phenologie_frt-bkg_2015_N:45-50.pdf}
  \captionT{Mesure du timing du bloom entre fronts et background}{%
    Les dates de démarrage du bloom et sa durée sont obtenues à partir des séries temporelles de la médiane de \as{chl}~(a) et de sa dérivé~(b). Les résultats pour les trois séries temporelles correspondant au \eng{background}~(rouge), fronts faibles~(bleu), et fronts forts~(vert) sont superposés; le timing du bloom pour chaque série est représenté par une flèche horizontale.
    Les données proviennent de l'année~2015 et la bande de latitude~\latlonRange{40}{45N}.
  }
  \label{fig:bloom-lag}
\end{figure}

Enfin, nous prenons comme date de fin du bloom la date la plus tardive pour laquelle la \as{chl} est supérieure à~\pct{75} de sa valeur maximale (étape\,\textcircled{3}, en vert, \nref{fig:methode-timing-bloom}a).
Pour obtenir une incertitude nous prenons ici l'écart"-type de l'ensemble des dates pour lesquelles la \as{chl} est comprise dans un intervalle de~\pct{5} autour du seuil (\ab{cad}~\pct{75\pm2.5}).
La durée~\as{duree} du bloom est prise entre les dates de démarrage et de fin ci"-dessus.

Il est légitime de questionner le détail de ces méthodes \encadra{d'autant plus pour certaines années ou le démarrage du bloom est étalé sur une longue période, ou fragmenté en plusieurs pics}, cependant on s'intéresse ici aux différences entre les valeurs trouvées dans les fronts et dans le \eng{background} (\nref{fig:bloom-lag}).
Étant donné que l'on effectue ces mesures de manière identique dans les deux cas, on s'attend néanmoins à trouver des différences de valeurs pertinentes.

Les variables retenues pour l'étude de la phénologie du bloom sont donc le retard au démarrage du bloom (\(\am{lag} = \am{onset}_\frt - \am{onset}_\bkg \), \enquote{lag} dans l'article), et l'écart de durée (\(\am{lag-duree} = \am{duree}_\frt - \am{duree}_\bkg\)).
Leurs valeurs annuelles (ainsi que les incertitudes associées) sont moyennées avec pondération.
Par exemple, pour le retard de valeur annuelle~\(\am{lag}_y\) et d'incertitude~\(\err{\am{lag}}_y\), en prenant comme pondération~\(w_y\):
\begin{equation}
  w_y = \frac 1 {1 + \err{\am{lag}}_y} ,
\end{equation}
on calcule les valeurs moyennes:
\begin{equation}
  \am{lag} = \frac {\sum_y w_y \am{lag}_y} {\sum_y w_y}
  \text{ et }
  \err{\am{lag}} = \sqrt{ \frac1{N-1} \frac {\sum_y w_y \paren{\am{lag}_y - \am{lag}}^2 } {\sum_y w_y} },
\end{equation}
avec~\(N\) le nombre d'années.
Le calcul est identique pour l'écart de durée~\as{lag-duree}.
