
\chapterlof{Méthodes}
\label{chp:methodes}

\minitoc%

\section{Données}
\label{sec:donnees}

Dans cette section sont décrit les jeux de données utilisés pour ce projet.
Le choix de ces données s'est avéré être une question importante et difficile dès le départ du projet.
Nous savions qu'il allait être nécessaire de combiner plusieurs jeux de données: la \ab{sst} avec la \ab{chl}, puis avec des données de composition de la communauté de phytoplancton.
Cela a donc informé le choix des données, ainsi que la création des outils nécessaires à la gestion de plusieurs sets de données.

Plusieurs options ont été considérées, notamment pour la \ab{sst}.
Elles sont reportées ci"-dessous, même si elles ne sont pas utilisées pour le reste des résultats.

Au long de ce chapitre (et ailleurs dans ce manuscript) les articles publiés correspondant aux jeux de données sont bien évidemment cités, mais sont également précisés les accès par lesquels nous avons obtenus les données.
Ces derniers sont cités en note de bas de page, et listés dans la \creftitle{bib:data} de la bibliographie~(\cpageref{bib:data}).

\subsection{Sea Surface Temperature (SST)}
\label{sec:donnees-sst}

petite explication des difficultés lié au choix sst ?
bonne rés, exactitude des fronts ?

\subsubsection{MODIS-1km}
\declareDataset{sst_modis}

Un des premiers jeux de données \ab{sst} à considérer est celui utilisé par \textcite{liu_2016}, dont l'étude consistait également à détecter des fronts de \ab{sst} et les colocaliser aux données de \ab{chl}.
Ces variables étant rarement distribuées sur des grilles à des résolutions kilométriques, il apparaît comme nécessaire de \enquote{re-grider} des produits L2 (données d'un capteur converties en variables géophysiques, disposées à la résolution de capture).

Pour le jeu de données intitulé \verb|MODIS-1km| dans la suite, il s'agit des données provenant du capteur \ab{modis}, à bord du satellite Aqua \parencite{kilpatrick_2015}.
Les données sont téléchargées depuis le \href{https://cmr.earthdata.nasa.gov/search/}{Common Metadata Repository (CMR)}\footfullcite{sst_modis}, en sélectionnant les swaths intersectant notre région d'étude, collectés de jour, et pour les courtes longueurs d'onde infrarouges (\qty{11}{\um}).
Pour ensuite regridder les données sur une grille kilométrique globale, j'ai repris les outils utilisés par \textcite{liu_2016}, à savoir les programmes présents dans le paquet de l'\href{https://oceandata.sci.gsfc.nasa.gov/ocssw}{\ab{ocssw}}, version \verb|v7.5|, qui est maintenu par l'Ocean Biology Processing Group et notamment distribué à travers l'outil \href{https://seadas.gsfc.nasa.gov/}{SeaDAS}.
Les swaths, maintenant tous sur la même grille spatiale peuvent être moyennés par date.

Cela permet donc d'obtenir des données à la limite de la résolution des capteurs, mais présente un certain nombre de désavantages majeurs.
C'est tout d'abord un travail important à réaliser: d'abord de téléchargement les fichiers L2 étant particulièrement lourds et nombreux (jusqu'à une dizaine pour une journée); ensuite de regrillage, qui à une résolution kilométrique demande une certaine puissance de calcul.
De plus, toutes les étapes décrites jusqu'ici ne concernent qu'un seul capteur.
Des étapes et calculs additionnels seraient nécessaires pour y combiner les données d'autres capteurs, ce qui pose des problèmes supplémentaires et se ferait non sans difficultés.
Même pour un capteur identique (\ab{modis} à bord de Terra \ab{par-ex}), il devient nécessaire de considérer des différences de calibrations, ainsi que de possibles artefacts lorsque l'on superpose plusieurs swaths (qui d'ailleurs ont été largement ignorés pour agréger les swaths sur une seule journée\dots).

Face à ces difficultés, nous avons donc décidé dans un premier temps d'en rester à un seul capteur malgré la couverture spatiale réduite, notamment à cause des nuages.
Il est également à noter qu'à ce niveau, peu d'actions ont été prises pour disqualifier les pixels nuageux.
Il en résulte des pixels aux valeurs visiblement erronées mais qui n'ont pas été masqués comme nuageux.
Cela se manifeste par exemple comme du bruit autour de nuages.
Il est difficile de les disqualifier correctement sans un bon algorithme de détection de nuage, ce qui nécessite encore un large travail.
Nous contournons le problème d'une manière similaire à celle de \textcite{liu_2016} en ne travaillant que sur des fenêtres de~\qtyproduct{100 x 100}{\km} avec une faible couverture nuageuse (\textless\qty{70}{\percent}), ce qui élimine une partie de ces cas problématiques.

Malgré le fait que l'on soit limité à un seul type de capteur, \ab{modis} présente l'avantage de mesurer concomitamment la \ab{sst} et la couleur de l'océan.
Cela permet d'obtenir les valeurs de \ab{chl} aux même pixels, en utilisant le même traitement avec le jeu de données accessible au \ab{cmems} \parencite{chl_modis}.

Il est néanmoins possible de trouver d'autres jeux de données permettant notre étude et nécessitant moins de travail (et donc de possible erreurs), ces dernières années ayant vu apparaître des données distribuées à des résolutions élevées comme c'est le cas dans le paragraphe suivant.

\subsubsection{MUR}
\declareDataset{sst_mur}

Le produit suivant, désigné ici \af{mur} est développé et distribué par le \ab{ghrsst} au JPL Physical Oceanography DAAC\footfullcite{sst_mur}.
Il présente les avantages d'être distribué à une résolution kilométrique et sans nuages grâce à une méthode d'interpolation par vaguelettes \parencite{chin_2017}.
Il intègre de nombreuses sources provenant de plusieurs capteurs infrarouges et micro"-ondes, ainsi que de mesures in-situ.
La grande couverture spatiale de ce produit se fait donc par l'utilisation de mesures non"-contraintes par la couverture nuageuse (\ab{cad} micro-ondes et in-situ), mais qui présente une résolution bien plus faible.
Le champ de \ab{sst} se retrouve donc lissé à la fois par l'interpolation et par l'inclusion de ces mesures.

L'utilisation du produit suivant cherche à palier à ce problème.

\subsubsection{ESA SST CCI / C3S}
\declareDataset{sst_esacci}

Ce produit est distribué conjointement par l'\ab{esa} \ab{sst} \ab{cci} et le \ab{c3s}.
Il agglomère les mesures de tous les capteurs infrarouges disponibles depuis 1981, ce qui comprend 11 \ab{avhrr} à bord des satellites \ab{metop}["~A] et \ab{noaa} (entre les itérations 6 et 19) et trois \ab{atsr} à bord de \ab{ers}~1 et~2, et Envisat.
Cela donne au moins deux capteurs en fonctionnement simultané, et au moins trois depuis 1992.
Outre la bonne couverture obtenue, l'uniformité des capteurs permet d'obtenir un jeu de données stable dans le temps, pouvant servir à la détection de tendance climatiques \parencite{merchant_2019}.

Les données sont distribuées à différents niveaux:
\begin{itemize}
  \item L2P mono-capteur pré-traité,
  \item L3U mono-capteur sur grille régulière,
  \item L3C multi-capteur (regroupé par famille d'instrument),
  \item L4 multi-capteur analysé.
\end{itemize}
La grille utilisée a une résolution de~\resol{1}{20} en latitude et longitude (soit~\qty{\approx5.6}{\km}).

Le produit L2P peut être utilisé de la même manière que pour les données \dataname{sst_modis}~(\datasect{sst_modis}) afin d'obtenir une résolution la plus haute possible, mais avec l'avantage de disposer de données uniformes sur un grand nombre de capteurs.
Il est à noter que les outils à utiliser pour regriller pourrait ne pas directement fonctionner sur ces données.

Les produits L3U et L3C peuvent être utilisés dans l'étude de structures fines et où la présence de nuages n'est pas rédhibitoire.

Enfin, le produit L4 regroupe l'ensemble des capteurs en utilisant le schéma d'intégration variationnelle \verb|NEMOVAR|, intégré dans le système \ab{ostia} \parencite{good_2020}.
Ce produit donne ainsi un champ de \ab{sst} sans nuages, estimé par une combinaison des observations satellites et de la prévision d'un modèle numérique d'un jour sur l'autre.
L'absence de couverture nuageuse et la simplicité d'usage se fait au détriment du lissage inévitable des structures les plus fines.

Une estimation quantitative du lissage due à l'interpolation est compliqué, car la paramétrisation dépend de la variabilité du champ de température à J"~1.
On peut néanmoins imaginer que le champ de \ab{sst} produit est plus lissé dans les zones normalement nuageuses, et potentiellement plus éloigné de la réalité (voir \cref{sec:donnees-sst-reanalyses}).
Il est tout de même possible de mitiger ces effets en ne comptabilisant dans nos résultats seulement
À noter qu'il serait également possible d'utiliser les erreurs d'intégration associées à chaque pixel, mais cela n'a pas été exploré.

Sauf indication contrainte, c'est le produit L4 que nous utiliserons dans toute la suite de cette thèse.
Il permet d'accéder à des données \ab{sst} simplement, avec une résolution convenable, et est stable sur une longue durée (plusieurs décennies).
Les données sont téléchargées depuis \ab{cmems}\footfullcite{sst_esacci}.

\subsubsection{Réanalyses}
\label{sec:donnees-sst-reanalyses}

Une solution pour s'affranchir de la couverture nuageuse est de s'appuyer sur des produits de réanalyses.
Tôt dans ce projet, certaines pistes de travail (\ab{par-ex} le suivi temporel des fronts) nous ont poussé à tester l'utilisation de tels produits.

% Problème ici: c'est très vieux (pendant stage de master), donc j'ai peu de traces.
% Mais visiblement j'utilisai \verb|GLOBAL_REANALYSIS_PHY_001_030| sur CMEMS\@.
% Il n'est plus dispo ou a changé de nom. Le successeur devrait être: \url{https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description}.
% C'est une réanalyse au 1/12° (ie 8~km). C'est beaucoup.
% Mais peut-être que la comparaison peut quand même valoir le cout ?

\begin{figure}
  \caption{Décalage entre réanalyses et observations satellites.}
  \label{fig:decalage-reanalyses}
\end{figure}

\subsection{Chlorophylle-\emph{a}}
\label{sec:donnees-chl}
\declareDataset{chl_modis}
\declareDataset{chl_globcolour}

Pour le champ de \al{chl}, nous utilisons les données produites dans le cadre du projet GlobColour, développées, validées et distribuées par ACRI"~ST, France \parencite{maritorena_2002}.
La version \enquote{MultiYear}, au niveau L3 est utilisée.
On obtient des données à une résolution de~\qty{4}{\km}, journalière, avec des nuages.
Les données sont récupérées sur \ab{cmems}\footfullcite{chl_globcolour}.

Ce produit aggrège les données optiques de plusieurs capteurs: \ab{seawifs}, \ab{modis} Aqua et Terra, \ab{meris} à bord d'Envisat, \ab{viirs} à bord de \ab{snpp} et \ab{noaa}[-20], et enfin \ab{olci} à bord de Sentinel-3A et -3B.
Les données de reflectance de chaque capteur sont transformées en concentration de \ab{chl} avant d'être fusionnées en seul produit.
Les algorithmes pour le passage en \ab{chl} sont ajustés pour chaque capteurs, ce qui permet d'obtenir un produit cohérent et stable \parencite{garnesson_2019}.


\subsection{Accord entre les produits}

Il est à noter qu'il est difficile de trouver des produits de \al{chl} à des résolutions supérieures à~\qty{4}{\km}, en tout cas au niveau global.
Cela motive à utiliser un produit \ab{sst} de résolution légèrement moindre afin d'éviter une étape supplémentaire de downsampling une fois les fronts repérés sur le champ de \ab{sst}.
Cela renforce notre choix pour les données \dataname{sst_esacci}, en défaveur du set \dataname{sst_mur}.

Les deux jeux choisis pour la \ab{sst} et \ab{chl} ont des résolutions spatiales similaires mais néanmoins légèrement différentes.
Les deux grilles sont plate"-carrées (régulières en latitude et longitude) mais le champ de \ab{chl} est défini sur une grille \glshref{epsg-chl} de résolution~\resol{1}{24}~(\qty{\approx4.6}{\km}); et le champ de \ab{sst} sur une grille \glshref{epsg-sst} de résolution~\resol{1}{20}~(\qty{\approx5.6}{\km}).

Sachant que les produits de composition du phytoplancton dont nous disposions (non décrits ici) sont définis sur la même grille que la Chlorophylle, nous adaptons la \ab{sst} cette grille par une simple interpolation bi-linéaire.

\subsection{Bathymétrie}
\label{sec:donnees-bathymetrie}
\declareDataset{etopo1}

Nous utilisons les données de bathymétrie ETOPO1\footfullcite{etopo1}, fournies par \ab{noaa}.
Les données sont distribuées sur une grille plate"-carrée à une résolution d'une minute d'arc~(\resol{1}{60}), soit trois fois trop pour nous.
Nous sous"-échantillonons les données en appliquant une moyenne à chaque groupe de 3\texttimes3 pixel, puis en interpolant le résultat sur la même grille que le reste des données de la même manière que pour la \ab{sst}.

\section{Délimitations des sous-régions d'étude}
\label{sec:delimitations-regions}

Comme détaillé plus en profondeur en introduction (\chapref{sec:region-detude}{chp:introduction}) notre région d'étude est fortement hétérogène, aussi bien concernant les propriétés physiques que biologiques.
Il est ainsi nécessaire de séparer notre zone d'étude en (sous-)régions afin d'extraire des résultats sur des zones homogènes.
Nous définissons donc trois biomes.
Le biome \ab{sb-perm}, le plus au sud, correspond à un régime largement oligotrophe.
Le biome \ab{sp}, le plus au nord, comprend les eaux froides et productives au nord du Gulf Stream.
Enfin le biome \ab{sb-sais}, entre les deux précédents, présente un régime intermédiaire: oligotrophe mais avec une production plus élevée permise par une couche de mélange profonde en hiver.
Dans cette section, nous décrivons plus précisement la méthode utilisée pour définir ces biomes spatialement.

\begin{figure}
  Snapshot SST et Chl avec contour des régions
  \caption[Séparation de la région d'étude en sous-régions]{
    Résultat de la séparation de la région d'étude en trois sous-régions.
  }
  \label{fig:separation-regions}
\end{figure}

La séparation entre les deux biomes subtropicaux est faite par une limite zonale fixée à~\ang{32}N (trait noir pointillé \cref{fig:separation-regions}).
Cette limite correspond à un saut visible des valeurs de \ab{chl} à cette latitude qui ne varie que peu au cours de l'année, ainsi qu'à l'isocontour~\qty{0.1}{\mgm} de la moyenne annuelle de \ab{chl}.
Cette séparation est en accord avec la limite entre les biomes présentés par \textcite{sarmiento_2004}.

On sépare ensuite le reste de la région au nord de~\ang{32}N en prenant comme limite le front nord du jet du Gulf Stream (le \enquote{North wall}).
Cette délimitation est donc dynamique et déterminée chaque jour à partir de l'image de température de la manière suivante.
En effet, il apparaît que la distribution de la \ab{sst} (au nord de~\ang{32}N) suffit à repérer de manière fiable et robuste une température seuil permettant de séparer les deux biomes (trait noir plein \cref{fig:separation-regions}).
Sur cette distribution apparaît clairement un pic dans des valeurs élevées correspondant au jet.
Il est aisé de repérer le pic et l'ajuster par une gaussienne.
À partir de là, la température seuil entre les biommes est prise comme la base froide de ce pic, \ab{cad} plus précisemment en soustrayant la température moyenne du pic  deux fois son écart"-type (\cref{fig:temp-seuil-distrib}).
La valeur journalière de ce seuil est filtrée temporellement par un filtre médian glissant (avec une fenêtre de largeur 8~jours) afin d'éviter d'éventuelles anomalies de détection.

\begin{figure}
  % \includegraphics[width=\textwidth]{zones.pdf}
  Histogramme des valeurs de SST, avec fit gaussien et threshold.
  \caption[Délimitation des biomes subtropical permanent et subpolaire par température seuil]{
    Détermination de la température de seuil séparant les biomes subtropical permanent et subpolaire.
    Sur la distribution des valeurs en température au nord de~\ang{32}N (trait plein noir), le pic (ici autour de~\tC{18}) correspond aux eaux du jet. Il est ajusté par un fit gaussien (en rouge).
    La température de seuil entre les deux biomes est <prise> comme la température moyenne du pic moins deux fois son écart"-type.
    Cela correspond à la limite nord du Gulf Stream.
  }
  \label{fig:temp-seuil-distrib}
\end{figure}

\begin{figure}
  \caption{Figure de Sarmiento 2004 ?}
  \label{fig:sarmiento}
\end{figure}

\begin{figure}
  Superposition de tous les contours de seuil pour une année.
  Tracé moyen ?
  \caption{Variation de la délimitation}
  \label{fig:var-delim}
\end{figure}

Par ailleurs, il est également nécessaire d'éviter de considérer les pixels trop près des côtes dans notre étude.
De manière générale la \ab{chl} y suit régime côtier, visible par des valeurs très élevées (\qty{>10}{\mgm}).
Ensuite, nous cherchons à éviter deux zones qui ne correspondent pas aux biomes définis plus haut.
Le jet du Gulf Stream prend forme au sud de notre zone, le long de la côte de Floride. On trouve donc à ces latitudes (\ang{\approx28}N) sur le plateau continental de forts courants qu'on ne retrouve pas dans le reste du biome \ab{sb-perm}.
Plus au nord dans l'anse Nord-Est Américaine de l'Atlantique (\enquote{Mid-Atlantic Bight}), on trouve une séparation nette entre les eaux au nord du Gulf Stream (la \enquote{slope sea}), et les eaux sur le plateau continental.
Un jet marque cette séparation le long du talus continental \parencite{flagg_2006}.

Dans les deux cas, imposer une limite haute à la bathymétrie sur notre région d'étude permet de supprimer ces zones problématiques.
Ainsi, pour calculer nos résultats, nous ne considérons que les pixels où la profondeur n'excède pas~\qty{1500}{\m}.
Nous utilisons pour cela les données de bathymétrie \dataname{etopo1} (voir~\datasect{etopo1}).

\section{Heterogeneity Index (HI)}
\label{sec:HI}

Comme précisé en introduction, pour quantifier l'effets des fronts sur le phytoplancton il est nécessaire de détecter les fronts, \ab{ie} classer chaque pixel comme appartenant à un front ou non (\ab{cad} à l'arrière"-plan ou \enquote{background}).
La méthode retenue ici suit celle présentée par \textcite{liu_2016} --- qui par son utilisation d'une fenêtre glissante s'apparente elle même à celle de \textcite{cayula_1992}.
Cette section défini cette méthode et notre implémentation, tout en indiquant les différences avec celle de \textcite{liu_2016}.

\subsection{Définition et implémentation}
\label{sec:HI-definition}

La méthode de \textcite{liu_2016} cherche à quantifier plusieurs valeurs statistiques du champ de \ab{sst} dont les fortes valeurs sont associées à la présence de fronts et autres structures de fine échelle.
Ces variables sont la bimodalité (à l'instar de la méthode de \textcite{cayula_1992}), l'écart"-type (qui reflète le gradient du champ), et le coefficient d'asymétrie~(\enquote{skewness}).

Pour limiter la taille des structures détectées, on limite le calcul de ces variables sur une fenêtre de taille appropriée, \ab{cad} pour nous de l'ordre de grandeur d'une dizaine de kilomètres.
Pour chaque pixel, la valeur des composantes citées précédemment est donc calculée sur la distribution de \ab{sst} à l'intérieur d'une fenêtre glissante, centrée sur ce pixel et dont les tailles possibles sont:
\begin{itemize}
        \item \(3 \times 3 =\qty{9}{\pixels}\) soit~\qty{17}{\km} de côté,
        \item \(5 \times 5 =\qty{25}{\pixels}\) soit~\qty{28}{\km} de côté,
        \item \(7 \times 7 =\qty{49}{\pixels}\) soit~\qty{39}{\km} de côté.
\end{itemize}
D'une part une fenêtre de taille trop limitée ne permet pas de calculer convenablement certaines des variables statistiques dont nous avons besoin, et c'est pourquoi nous ne considérons pas de fenêtres plus petites.
D'autre part une fenêtre trop large entraînera la détection de structures trop grandes, et nous nous limiterons donc aux tailles présentées ci-dessus.

Nous définissons maintenant plus en détails le calcul des composantes du \ab{hi}.
Pour chaque position de la boite, on regarde pour les \(N\) valeurs de \ab{sst} \(s_{i}\) valides (\ab{cad} sans nuages).

On commence par l'écart"-type~\ab{std}, calculé simplement par:
\begin{equation}
  \am{std} = \sqrt{\frac{1}{N-1} \sum_i \paren{s_i - \moy{s}}^2},
\end{equation}
avec \(\moy{s}\) la moyenne des valeurs de \ab{sst}.

Ensuite, le coefficient d'asymétrie~\ab{skew}, défini comme le moment d'ordre trois d'une variable centrée réduite, se calcule:
\begin{equation}
  \am{skew} = \frac{\sum_i \paren{s_i - \moy{s}}^3} {N \sigma^3}.
\end{equation}

Enfin, on cherche à quantifier la bimodalité~\ab{bimod} de la distribution des valeurs de \ab{sst}.
Pour ce faire on compare l'écart entre ladite distribution et une distribution gaussienne de moyenne et d'écart"-type identiques comme illustré sur la \cref{fig:bimodality}.
Cela présuppose que lorsque les températures sont uni"-modales (\ab{cad} quand il n'y pas de fronts dans la fenêtre) leur distribution tend vers une gaussienne, ce qui ne paraît pas déraisonnable.

\begin{figure}
  \centering
  % \includegraphics[width=\textwidth]{méthodes/bimodality.pdf}
  \caption[Illustration du calcul de la bimodalité]{
    On calcule la bimodalité comme la norme de la différence entre l'histogramme des températures dans la fenêtre (trait noir) et une distribution gaussienne de même moyenne et de même écart"-type (trait rouge).
  }
  \label{fig:bimodality}
\end{figure}

De manière plus précise on commence par calculer l'histogramme~\(h_i\) des valeurs de \ab{sst} dans la fenêtre, en utilisant des intervalles de largeur fixe de~\qty{0.1}{\dC} et compris entre les valeurs minimales et maximales dans la fênetre.
Les données de \ab{sst} étant stockées compressées par \enquote{linear packing}\footnotemark\ avec un facteur d'échelle de~\qty{0.01}{\dC}, nos intervalles ont une largeur précisement égale à dix fois l'écart minimal possible entre deux valeurs de température.
Pour éviter que trop de valeurs tombent sur les bords des intervalles et que l'histogramme soit pollué par des erreurs numériques, on décale les intervalles de~\qty[parse-numbers=false]{0.01/2}{\dC}.
Par ailleurs, le nombre d'intervalles étant dépendant de la largeur de la distribution de \ab{sst}, dans les cas où il est inférieur ou égal à quatre, la bimodalité est automatiquement assignée nulle (\(B=0\)).
\footnotetext{%
  Cette technique de compression avec pertes ---~utilisée notamment par l'outil \citesoft{nco}\footnotemark{}~--- consiste à discrétiser des valeurs flottantes sur des entiers après une transformation linéaire.
  Par exemple, en prenant pour stockage des entiers non"-signés sur 16~bits~(\texttt{NC\_SHORT}) on peut évidement représenter des valeurs entières entre 0 et \(2^{16}-1 =\num{65535}\); mais en multipliant ces valeurs entières par un facteur de, disons,~\num{0.005} on peut représenter des valeurs entre 0 et~\num{327.675}.
  On a gagné en volume par rapport à un stockage typique de~32 ou~64~bits, mais en perdant évidemment en précision puisque nos valeurs sont maintenant discrétisées avec un intervalle de~\num{0.005}.}
\footnotetext{voir le guide utilisateur: \glsurl{nco-packed}}

Par ailleurs on définit une distribution gaussienne~\(g_i\) sur les même intervalles en utilisant les statistiques calculées précédemment:
\begin{equation}
  g_i = \frac{1}{\sqrt{2\pi\am{std}}} \exp\paren{-\frac{\paren{x_i-\moy{s}}^2}{2\am{std}^2}},
\end{equation}
pour ensuite calculer la norme \(\mathbb{L}^2\) entre les deux distributions:
\begin{equation}
  \begin{split}
  \am{bimod} & = \norme[2]{h - g}\\
             & = \sum_i \paren{h_i - g_i}^2 .
  \end{split}
\end{equation}

Le principe de calcul de la bimodalité s'apparente à celle de \textcite{cayula_1992}, qui consiste à trouver la température seuil séparant l'histogramme en deux classe et pour laquelle la variance intra"-classe est minimale.
Cette méthode de séparation par seuil est d'ailleurs utilisée en analyse d'image, non pas pour une fenêtre glissante mais pour toute l'image, et connue sous le nom de méthode d'\textcite{otsu_1979}.
Bien qu'elle soit appliquée avec succès pour la détection de front, il apparaît que cette méthode nécessite d'avoir un histogramme d'une résolution suffisante ce qui s'avère difficile aux échelles où nous travaillons.

Notre méthode de calcul de la bimodalité est la première grande modification apportée à celle de \textcite{liu_2016}.
Ces dernier·ère·s calculent également l'histogramme de la température. Au lieu de directement le comparer avec une distribution gaussienne, iels ajustent l'histogramme par un polynôme de degré~5 avant de comparer ce dernier avec la gaussienne.
Cet ajustement, par ailleurs difficile à mettre en place, est facilement mal conditionné et rien ne garantit sa convergence.

Avant de pouvoir réunir les trois composantes il est nécessaire de calculer pour chacune un coefficient constant de normalisation, afin que chaque composante ait le même poids statistique.
Alors que \textcite{liu_2016} proposent de normaliser chaque composante \(C^n\) par ses valeurs minimales et maximales (prises sur toutes les valeurs disponibles) selon:
\begin{equation}
  \norm{C_i^n} = \frac {C_i^n - \min(C^n)} {\max(C^n) - \min(C^n)} ;
\end{equation}
nous préférons plutôt normaliser par l'écart"-type:
\begin{equation}
  \norm{C_i^n} = \am{coef}^n C_i^n
  \text{ avec } \am{coef}^n = \frac {1} {\std(C^n)} .
\end{equation}
En effet un rapide coup d’œil aux distributions des composantes (voir \cref{fig:distrib-composantes}) permet de constater qu'aucune des composante ne semble avoir des valeurs bornées.
Normaliser par les valeurs maximales donne d'une part une normalisation arbitraire, très sensible aux valeurs extrêmes, et d'autre part donne un poids disproportionné aux valeurs élevées.
En revanche, la normalisation par l'écart"-type permet d'attribuer une part équivalente de la variance du \ab{hi} à chaque composante.

\begin{figure}
  \caption[Distribution des composantes du \glsentryshort{hi} et du \glsentryshort{hi}]{Distribution des trois composantes du \ab{hi}, ainsi que du \ab{hi}.}
  \label{fig:distrib-composantes}
\end{figure}

La normalisation par écart"-type n'impose aucune contrainte sur l'amplitude des valeurs.
Afin de \enquote*{standardiser} quelque peu les valeurs finales du \ab{hi}, nous assignons un quatrième coefficient de normalisation défini de sorte que~\pct{95} des valeurs du \ab{hi} soient inférieures à~\num{9.5}.

Les coefficients de normalisation~\(\am{coef}^n\) sont obtenus par analyse des distributions des composantes pour l'année~2007 uniquement. Ils sont ensuite appliqués de manière uniforme au reste des données.

On peut donc enfin calculer le \ab{hi} avec:
\begin{equation}
  \am{hi} = \am{coef}^4 \paren{
    \am{coef}^1\am{std}
    + \am{coef}^2\am{skew}
    + \am{coef}^3\am{bimod}}.
\end{equation}

\subsection{Sensibilité aux paramètres}
\label{sec:HI-sensibilite}

Nécessité d'évaluer les incertitudes sur la méthode.
Pour voir si résultat significatif.
Taille de la fenêtre glissante. Coefs de normalisation.

\subsection{Exemples de fronts}
\label{sec:HI-exemples}

Sélectionner des images ou ça marche (ou pas).
Préférentiellement avec couverture MODIS mais pas obligé.

\section{Extraction des résultats}
\label{sec:extraction-res}

\subsection{Utilisation d'histogrammes}
\label{sec:extraction-hist}

Une fois le \ab{hi} calculé, il devient possible de catégoriser chaque pixel par la région à laquelle il appartient (subtropical permanent, subtropical saisonnier, subpolaire), ainsi que par sa valeur de \ab{hi}.
On cherche donc à extraire des informations de ces ensembles de pixels ainsi consistués.
Le nombre total de pixels étant conséquent, et parce que l'établissement des ensembles est compliqué et coûteux (il faut superposer la \ab{sst}, le \ab{hi}, et la \ab{chl}), les ensembles de pixels sont chacun réduits à des histogrammes des variables d'intérêt (\ab{sst}, \ab{chl}, les \abp{pft}).
Cela diminue les étapes de calcul ainsi que la quantité de données à traiter pour obtenir un diagnostic: on passe de~\numproduct{1000 x 1000} pixels à~\num{\approx100} intervalles d'histogramme.

Les histogrammes peuvent être rendus représentatifs sans pour autant utiliser un nombre prohibitif d'intervalles, d'autant plus que les données (\ab{chl} et \ab{sst}) sont déjà stockées compressées avec pertes. Pour la température par exemple, 450 intervalles suffisent à couvrir toutes les valeurs (de~\qty{-5}{\dC} à~\qty{40}{\dC}), avec une largeur d'intervalle de~\qty{0.1}{\dC} égale à l'intervalle de discrétisation des données, lui-même équivalent à l'incertitude sur la mesure.
Les intervalles pour la \ab{chl} et les autres variables biologiques sont pris de largeur logarithmique afin de couvrir les plusieurs ordres de grandeur que peuvent prendre leurs valeurs.

Les histogrammes présentent également l'avantage de pouvoir facilement être combinés entre eux.
En effet, tous les histogrammes calculés sont stockés non"-normalisés, c'est-à-dire en nombre de pixel par intervalle. Ainsi plusieurs histogrammes peuvent être sommés entre eux avant d'être normalisés pour en extraire une valeur, comme la valeur médiane de la distribution résultante par example.
Ce procédé est notamment utilisé pour calculer des diagnostics sur des périodes de temps autres que journalières, sans avoir besoin de refaire un calcul coûteux impliquant les pixels.
Dans la suite on considère un histogramme qui compte~\(h_i\) valeurs d'une variable quelconque~\(x\), pour le i\ieme{} intervalle \(\left[x_i; x_{i+1} \right]\).

Pour un histogramme donné (\as{par-ex} pour une région à une date et un type de front donné), on extrait facilement le nombre de pixels total
\begin{equation}
  N = \sum_i h_i,
\end{equation}
ou une approximation de la valeur moyenne
\begin{equation}
  \moy{x} = \frac{\sum_i h_i x_i} {N}.
\end{equation} % ceci est un test

Pour d'autres valeurs à extraire il est nécessaire de normaliser les histogrammes afin d'obtenir une densité de probabilité. On prendra soin de considérer les tailles des intervalles dans les calculs.
La largeur des intervalles est \(w_i = x_{i+1}-x_i\). On transforme le nombre de points en probabilité par unité de valeur \(p_i = h_i / w_i \), avant de le normaliser de sorte à obtenir une intégrale égale à 1:
\begin{equation}
  f_i = \frac{p_i} {\sum_j p_j w_j},
\end{equation}
pour ainsi obtenir une approximation de la densité de probabilité \(f\).

On peut extraire de cette distribution notamment la médiane, ainsi que des percentiles divers en trouvant la valeur de \(x\) pour laquelle la somme cumulée de la densité de probabilité est égale au percentile recherché (0.5 dans le cas de la médiane).
On délègue ce travail au paquet SciPy dont l'objet \glshref{rv_histogram} permet d'effectuer ces calculs de manière triviale.

Vérification que les histogrammes sont well-behaved pour prendre des valeurs.

\subsection{Quantification de l'effet des fronts}
\label{sec:extraction-surplus}

Afin de comparer les valeurs de diverse variables à l'intérieur et à l'extérieur des fronts, nous définissons deux métriques.
La première est l'excès~\ab{exces} de \ab{chl} (\enquote{excess} dans l'article \chapref{sec:article-bg}{chp:res-chl}), qui compare localement les valeurs dans les fronts et le background. On le définit simplement comme la différence relative entre la médiane de \ab{chl} dans et hors des fronts:
\begin{equation}
  \am{exces} = \frac{\med{\am{chl}}_\frt - \med{\am{chl}}_\bkg}
  {\med{\am{chl}}_\bkg} .
\end{equation}
Le calcul est fait pour les fronts faibles et forts de la même manière, le background désignant les pixels de \ab{hi} faible (\(\am{hi} < 5\)).
L'excès est calculé dans des bandes de latitudes larges de~\ang{5}, afin de minimiser l'influence des gradients de grande échelle.

Cette métrique ne tient compte que de la distribution des valeurs de \ab{chl}, et ne représente pas la quantité totale présente ou la proportion de fronts dans une zone donnée.
Ainsi on définit le surplus~\ab{surplus} de \ab{chl} (\enquote{biome surplus} dans l'article \chapref{sec:article-bg}{chp:res-chl}) dans toute un biome comme la différence relative entre la moyenne de \ab{chl} dans tout un biome et la moyenne dans le background:
\begin{equation}
  \am{surplus} = \frac{\moy{\am{chl}}_\tot - \moy{\am{chl}}_\bkg}
  {\moy{\am{chl}}_\bkg} .
\end{equation}

\section{Références informatiques}
\label{sec:ref-info}

J'ai été amené à utiliser durant ma thèse de nombreux outils informatiques sans lesquels ce travail n'aurait pu aboutir, ou en tout cas avec assurément beaucoup plus de difficultés.
Il me semble important d'en citer au moins une partie ici.
La reproductibilité de mon travail est déjà garantie par la mise à disposition de mes codes (accompagnés d'une courte documentation et de tout le nécessaire pour reproduire les résultats) sur un dépôt public\footnote{%
  Dépot Gitlab: \glsurl{gitlab}}
et dont une version est également déposée sur un répertoire Zenodo\footnote{\glsurl{zenodo}}.
Il s'agit ici plutôt de créditer les nombreux contributeurs qui ont participé à l'élaboration de ces outils. Les outils mentionnés ici (et ailleurs dans ce manuscript) sont listés dans la \creftitle{bib:software} de la bibliographie~(\cpageref{bib:software}).

Bien évidemment beaucoup des calculs reposent sur des librairies qui ne sont plus à présenter: \citesoft{numpy}, \citesoft{scipy}, et \citesoft{pandas}.
Cependant il est plus commun (en géosciences en tout cas) d'interfacer avec la librairie \citesoft{xarray}.
Les outils de \citesoft{dask} sont également avérés indispensables pour gérer les quantités importantes de données que sont les notres.

Il convient de citer plusieurs paquets permettant le calcul efficace d'histogrammes:
\citesoft{xhistogram} qui implémente ses fonctionalités à partir de fonctions NumPy et Dask \enquote*{élémentaires}, et
\citesoft{dask-histogram} qui s'appuie sur la librairie C \citesoft{boost}.

Plusieurs paquets sont utilisés pour réaliser les figures, bien évidemment \citesoft{matplotlib},
\citesoft{cartopy} pour les cartes,
mais également \citesoft{cmocean} qui fourni des palettes de couleurs, notamment pour un usage en océanographie. Ces dernières présentent les avantages (majeurs) d'être linéairement perceptibles, adaptés à une conversion en nuance de gris pour impression, et robustes à plusieurs types de daltonisme.
Similairement, les couleurs utilisées dans les figures sont le fruit du travail de Paul Tol\footnote{voir~\glsurl{paultol}}, distribué pour Python par mes soins (voir~\creftitle{sec:productions}).

Enfin il me parait approprié de citer les outils \citesoft{ipython} avec lequel j'execute tous mes scripts, \citesoft{mamba} et le projet \citesoft{conda-forge} lesquels me permettent de gérer les environments Python, et enfin le travail épatant derrière le framework de configuration \citesoft{doom} sur lequel tous les scripts et ce manuscript (entre autres) ont été écrits.
